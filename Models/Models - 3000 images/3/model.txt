from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

def model_definition(num_expressions):
    exp_model = Sequential()
    exp_model.add(Conv2D(input_shape=(48, 48, 1), filters=64, kernel_size=(3, 3), activation='relu', padding='same'))
    exp_model.add(BatchNormalization())
    exp_model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))
    exp_model.add(BatchNormalization())
    exp_model.add(MaxPooling2D())

    exp_model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))
    exp_model.add(BatchNormalization())
    exp_model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))
    exp_model.add(BatchNormalization())
    exp_model.add(MaxPooling2D())

    exp_model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))
    exp_model.add(BatchNormalization())
    exp_model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))
    exp_model.add(BatchNormalization())
    exp_model.add(MaxPooling2D())

    exp_model.add(Flatten())
    
    # Fully connected layers
    exp_model.add(Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    exp_model.add(BatchNormalization())
    exp_model.add(Dropout(0.5))

    exp_model.add(Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    exp_model.add(BatchNormalization())
    exp_model.add(Dropout(0.5))

    exp_model.add(Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    exp_model.add(Dense(num_expressions, activation='softmax'))

    return exp_model
