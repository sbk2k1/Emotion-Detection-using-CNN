def model_definition(num_expressions):
    exp_model = Sequential()
    exp_model.add(Conv2D(input_shape = (48,48,1), filters=64, kernel_size=(3,3), activation='relu', data_format='channels_last',padding='same'))
    exp_model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', data_format='channels_last',padding='same'))
    #exp_model.add(BatchNormalization())
    exp_model.add(MaxPool2D())
    #exp_model.add(Dropout(0.5))

    exp_model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', data_format='channels_last',padding='same'))
    #exp_model.add(BatchNormalization())

    exp_model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', data_format='channels_last',padding='same'))
    exp_model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', data_format='channels_last',padding='same'))
    exp_model.add(BatchNormalization())
    exp_model.add(MaxPool2D())
    exp_model.add(Dropout(0.5))
    
    exp_model.add(MaxPool2D())

    exp_model.add(Flatten())

    # Fully connected layer 1st layer
    exp_model.add(Dense(512, activation="relu"))
    exp_model.add(BatchNormalization())
    #exp_model.add(Activation('relu'))
    # Fully connected layer 2nd layer
    exp_model.add(Dense(256, activation='relu'))
    exp_model.add(BatchNormalization())
    #exp_model.add(Activation('relu'))

    exp_model.add(Dense(128, activation='relu'))
    exp_model.add(Dense(num_expressions, activation="softmax"))

    return exp_model
